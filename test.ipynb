{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:30.584594Z",
     "end_time": "2023-04-24T17:56:30.600840Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0341, 0.1362, 0.0033,  ..., 0.3346, 0.5185, 0.7874],\n         [0.9969, 0.1972, 0.6956,  ..., 0.4129, 0.0346, 0.0019],\n         [0.8931, 0.1291, 0.1384,  ..., 0.2164, 0.9745, 0.0144],\n         ...,\n         [0.7110, 0.4455, 0.4876,  ..., 0.6640, 0.5244, 0.4332],\n         [0.1466, 0.4007, 0.8518,  ..., 0.9688, 0.1019, 0.0836],\n         [0.0644, 0.1215, 0.6654,  ..., 0.3369, 0.7273, 0.1511]],\n\n        [[0.7475, 0.0039, 0.2339,  ..., 0.7669, 0.4143, 0.9183],\n         [0.2175, 0.1565, 0.8431,  ..., 0.3169, 0.2619, 0.8918],\n         [0.5553, 0.0379, 0.0761,  ..., 0.9414, 0.2924, 0.4765],\n         ...,\n         [0.4597, 0.8613, 0.6634,  ..., 0.6211, 0.0931, 0.3899],\n         [0.0848, 0.9119, 0.0465,  ..., 0.4028, 0.5934, 0.6574],\n         [0.8787, 0.0676, 0.3846,  ..., 0.7974, 0.0655, 0.1055]],\n\n        [[0.1686, 0.9004, 0.5451,  ..., 0.6909, 0.4324, 0.5653],\n         [0.5289, 0.2869, 0.8805,  ..., 0.8531, 0.5850, 0.5939],\n         [0.4629, 0.2602, 0.5822,  ..., 0.6829, 0.5096, 0.0946],\n         ...,\n         [0.5912, 0.1798, 0.3877,  ..., 0.9353, 0.2894, 0.9242],\n         [0.2179, 0.8178, 0.7072,  ..., 0.2816, 0.5812, 0.3323],\n         [0.7452, 0.9392, 0.0293,  ..., 0.4432, 0.4131, 0.8993]],\n\n        ...,\n\n        [[0.3947, 0.4560, 0.3128,  ..., 0.9944, 0.4963, 0.7172],\n         [0.0288, 0.7111, 0.3720,  ..., 0.2495, 0.9780, 0.4968],\n         [0.1519, 0.7383, 0.6103,  ..., 0.4521, 0.4877, 0.5252],\n         ...,\n         [0.3343, 0.2097, 0.6669,  ..., 0.7509, 0.0042, 0.6954],\n         [0.5996, 0.1021, 0.8153,  ..., 0.1537, 0.1160, 0.1914],\n         [0.8011, 0.3292, 0.1021,  ..., 0.3452, 0.0897, 0.6619]],\n\n        [[0.0660, 0.1030, 0.5050,  ..., 0.8574, 0.9053, 0.9341],\n         [0.7359, 0.6724, 0.4730,  ..., 0.2956, 0.6861, 0.9047],\n         [0.6309, 0.7340, 0.5145,  ..., 0.9431, 0.2354, 0.6720],\n         ...,\n         [0.5045, 0.4996, 0.5069,  ..., 0.2365, 0.8898, 0.0618],\n         [0.1344, 0.2449, 0.4981,  ..., 0.6123, 0.8946, 0.3090],\n         [0.0796, 0.2730, 0.4722,  ..., 0.6926, 0.6187, 0.1137]],\n\n        [[0.0777, 0.2894, 0.7696,  ..., 0.4136, 0.9458, 0.1440],\n         [0.0280, 0.2198, 0.1167,  ..., 0.4670, 0.6481, 0.3785],\n         [0.1729, 0.9055, 0.7515,  ..., 0.5082, 0.8675, 0.7865],\n         ...,\n         [0.5713, 0.2725, 0.4964,  ..., 0.2432, 0.8460, 0.3095],\n         [0.6025, 0.6953, 0.1275,  ..., 0.8010, 0.3672, 0.9224],\n         [0.5188, 0.6558, 0.5418,  ..., 0.9623, 0.4245, 0.5921]]],\n       device='cuda:0')"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att = torch.nn.MultiheadAttention(64, 8, 0.1, batch_first=True).cuda()\n",
    "batch = torch.rand(10, 1024, 8 * 8).cuda()\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:30.600840Z",
     "end_time": "2023-04-24T17:56:30.643995Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "q = k = v = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:30.630971Z",
     "end_time": "2023-04-24T17:56:30.675523Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "del q\n",
    "del k\n",
    "del v\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:30.646865Z",
     "end_time": "2023-04-24T17:56:30.726389Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "%%time\n",
    "q = k = v = torch.flatten(batch, 2).cuda()\n",
    "att(q, k, v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 6.00 GiB total capacity; 4.93 GiB already allocated; 0 bytes free; 5.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:4\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1189\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1175\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1176\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1177\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1186\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1187\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1189\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1192\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1193\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1198\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\functional.py:5299\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   5297\u001B[0m     attn_output_weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbaddbmm(attn_mask, q_scaled, k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m   5298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 5299\u001B[0m     attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5300\u001B[0m attn_output_weights \u001B[38;5;241m=\u001B[39m softmax(attn_output_weights, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   5301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dropout_p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 320.00 MiB (GPU 0; 6.00 GiB total capacity; 4.93 GiB already allocated; 0 bytes free; 5.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = torch.rand(10, 1024, 8 * 8).cuda()\n",
    "k = torch.rand(10, 1024, 8 * 8).cuda()\n",
    "v = torch.rand(10, 1024, 8 * 8).cuda()\n",
    "att(q, k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:32.368114Z",
     "end_time": "2023-04-24T17:56:32.568977Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T17:56:32.568977Z",
     "end_time": "2023-04-24T17:56:32.584976Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
